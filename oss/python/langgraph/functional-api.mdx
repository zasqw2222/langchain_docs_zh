---
title: Functional API 概述
sidebarTitle: Functional API
---



**Functional API** 允许您以最少的代码更改将 LangGraph 的关键功能——[持久化](/oss/python/langgraph/persistence)、[内存](/oss/python/langgraph/add-memory)、[人在回路](/oss/python/langgraph/interrupts)和[流式传输](/oss/python/langgraph/streaming)——添加到您的应用程序中。

它旨在将这些功能集成到可能使用标准语言原语进行分支和控制流的现有代码中，例如 `if` 语句、`for` 循环和函数调用。与许多需要将代码重构为显式管道或 DAG 的数据编排框架不同，Functional API 允许您合并这些功能，而无需强制执行严格的执行模型。

Functional API 使用两个关键构建块：

* **`@entrypoint`** – 将函数标记为工作流的起点，封装逻辑并管理执行流，包括处理长时间运行的任务和中断。
* **[`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task)** – 表示一个离散的工作单元，例如 API 调用或数据处理步骤，可以在入口点内异步执行。任务返回一个类似 future 的对象，可以异步等待或同步解析。




这为构建具有状态管理和流式传输的工作流提供了最小的抽象。

<Tip>
有关如何使用功能 API 的信息，请参阅[使用 Functional API](/oss/python/langgraph/use-functional-api)。
</Tip>

## Functional API vs. Graph API

对于喜欢更声明式方法的用户，LangGraph 的 [Graph API](/oss/python/langgraph/graph-api) 允许您使用图范式定义工作流。两种 API 共享相同的底层运行时，因此您可以在同一应用程序中一起使用它们。

以下是一些关键区别：

* **控制流**：Functional API 不需要考虑图结构。您可以使用标准 Python 构造来定义工作流。这通常会减少您需要编写的代码量。
* **短期内存**：**GraphAPI** 需要声明[**State**](/oss/python/langgraph/graph-api#state)，并且可能需要定义[**reducers**](/oss/python/langgraph/graph-api#reducers)来管理图状态的更新。`@entrypoint` 和 `@tasks` 不需要显式状态管理，因为它们的状态作用域到函数，不会在函数之间共享。
* **检查点**：两种 API 都生成和使用检查点。在 **Graph API** 中，每个[superstep](/oss/python/langgraph/graph-api)之后都会生成一个新的检查点。在 **Functional API** 中，当任务执行时，它们的结果会保存到与给定入口点关联的现有检查点，而不是创建新的检查点。
* **可视化**：Graph API 可以轻松地将工作流可视化为图，这对于调试、理解工作流和与他人共享很有用。Functional API 不支持可视化，因为图是在运行时动态生成的。

## 示例

下面我们演示一个简单的应用程序，它写一篇文章并[中断](/oss/python/langgraph/interrupts)以请求人工审查。

```python
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import interrupt

@task
def write_essay(topic: str) -> str:
    """Write an essay about the given topic."""
    time.sleep(1) # A placeholder for a long-running task.
    return f"An essay about topic: {topic}"

@entrypoint(checkpointer=InMemorySaver())
def workflow(topic: str) -> dict:
    """A simple workflow that writes an essay and asks for a review."""
    essay = write_essay("cat").result()
    is_approved = interrupt({
        # Any json-serializable payload provided to interrupt as argument.
        # It will be surfaced on the client side as an Interrupt when streaming data
        # from the workflow.
        "essay": essay, # The essay we want reviewed.
        # We can add any additional information that we need.
        # For example, introduce a key called "action" with some instructions.
        "action": "Please approve/reject the essay",
    })

    return {
        "essay": essay, # The essay that was generated
        "is_approved": is_approved, # Response from HIL
    }
```




<Accordion title="详细说明">
  此工作流将写一篇关于主题"cat"的文章，然后暂停以获取人工审查。工作流可以无限期中断，直到提供审查。

  当工作流恢复时，它从最开始执行，但由于 `writeEssay` 任务的结果已经保存，任务结果将从检查点加载，而不是重新计算。

  ```python
  import time
  import uuid
  from langgraph.func import entrypoint, task
  from langgraph.types import interrupt
  from langgraph.checkpoint.memory import InMemorySaver


  @task
  def write_essay(topic: str) -> str:
      """Write an essay about the given topic."""
      time.sleep(1)  # This is a placeholder for a long-running task.
      return f"An essay about topic: {topic}"

  @entrypoint(checkpointer=InMemorySaver())
  def workflow(topic: str) -> dict:
      """A simple workflow that writes an essay and asks for a review."""
      essay = write_essay("cat").result()
      is_approved = interrupt(
          {
              # Any json-serializable payload provided to interrupt as argument.
              # It will be surfaced on the client side as an Interrupt when streaming data
              # from the workflow.
              "essay": essay,  # The essay we want reviewed.
              # We can add any additional information that we need.
              # For example, introduce a key called "action" with some instructions.
              "action": "Please approve/reject the essay",
          }
      )
      return {
          "essay": essay,  # The essay that was generated
          "is_approved": is_approved,  # Response from HIL
      }


  thread_id = str(uuid.uuid4())
  config = {"configurable": {"thread_id": thread_id}}
  for item in workflow.stream("cat", config):
      print(item)
  # > {'write_essay': 'An essay about topic: cat'}
  # > {
  # >     '__interrupt__': (
  # >        Interrupt(
  # >            value={
  # >                'essay': 'An essay about topic: cat',
  # >                'action': 'Please approve/reject the essay'
  # >            },
  # >            id='b9b2b9d788f482663ced6dc755c9e981'
  # >        ),
  # >    )
  # > }
  ```

  An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:

  ```python
  from langgraph.types import Command

  # Get review from a user (e.g., via a UI)
  # In this case, we're using a bool, but this can be any json-serializable value.
  human_review = True

  for item in workflow.stream(Command(resume=human_review), config):
      print(item)
  ```

  ```pycon
  {'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}
  ```

  The workflow has been completed and the review has been added to the essay.



</Accordion>

## 入口点

[`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) 装饰器可用于从函数创建工作流。它封装工作流逻辑并管理执行流，包括处理_长时间运行的任务_和[中断](/oss/python/langgraph/interrupts)。




### 定义

**入口点**通过使用 `@entrypoint` 装饰器装饰函数来定义。

函数**必须接受单个位置参数**，该参数用作工作流输入。如果您需要传递多个数据，请使用字典作为第一个参数的输入类型。

使用 `entrypoint` 装饰函数会产生一个 [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) 实例，该实例有助于管理工作流的执行（例如，处理流式传输、恢复和检查点）。

您通常希望将**检查点保存器**传递给 `@entrypoint` 装饰器以启用持久化并使用**人在回路**等功能。

<Tabs>
    <Tab title="Sync">
    ```python
    from langgraph.func import entrypoint

    @entrypoint(checkpointer=checkpointer)
    def my_workflow(some_input: dict) -> int:
        # some logic that may involve long-running tasks like API calls,
        # and may be interrupted for human-in-the-loop.
        ...
        return result
    ```
    </Tab>
    <Tab title="Async">
    ```python
    from langgraph.func import entrypoint

    @entrypoint(checkpointer=checkpointer)
    async def my_workflow(some_input: dict) -> int:
        # some logic that may involve long-running tasks like API calls,
        # and may be interrupted for human-in-the-loop
        ...
        return result
    ```
    </Tab>
</Tabs>




<Warning>
**序列化**
入口点的**输入**和**输出**必须是 JSON 可序列化的，以支持检查点。有关更多详细信息，请参阅[序列化](#serialization)部分。
</Warning>

### 可注入参数

声明 `entrypoint` 时，您可以请求访问将在运行时自动注入的其他参数。这些参数包括：

| 参数       | 描述                                                                                                                                                        |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **previous** | 访问与给定线程的上一个 `checkpoint` 关联的状态。请参阅[短期内存](#short-term-memory)。                                                                      |
| **store**    | [BaseStore][langgraph.store.base.BaseStore] 的实例。对于[长期内存](/oss/python/langgraph/use-functional-api#long-term-memory)很有用。                                  |
| **writer**   | 在使用 Async Python < 3.11 时用于访问 StreamWriter。有关详细信息，请参阅[使用功能 API 进行流式传输](/oss/python/langgraph/use-functional-api#streaming)。         |
| **config**   | 用于访问运行时配置。有关信息，请参阅 [RunnableConfig](https://python.langchain.com/docs/concepts/runnables/#runnableconfig)。                                  |

<Warning>
使用适当的名称和类型注释声明参数。
</Warning>

<Accordion title="Requesting Injectable Parameters">
  ```python
  from langchain_core.runnables import RunnableConfig
  from langgraph.func import entrypoint
  from langgraph.store.base import BaseStore
  from langgraph.store.memory import InMemoryStore

  in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory

  @entrypoint(
      checkpointer=checkpointer,  # Specify the checkpointer
      store=in_memory_store  # Specify the store
  )
  def my_workflow(
      some_input: dict,  # The input (e.g., passed via `invoke`)
      *,
      previous: Any = None, # For short-term memory
      store: BaseStore,  # For long-term memory
      writer: StreamWriter,  # For streaming custom data
      config: RunnableConfig  # For accessing the configuration passed to the entrypoint
  ) -> ...:
  ```
</Accordion>


### 执行

使用 [`@entrypoint`](#entrypoint) 会产生一个 [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) 对象，可以使用 `invoke`、`ainvoke`、`stream` 和 `astream` 方法执行。

<Tabs>
    <Tab title="Invoke">
    ```python
    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }
    my_workflow.invoke(some_input, config)  # Wait for the result synchronously
    ```
    </Tab>
    <Tab title="Async Invoke">
    ```python
    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }
    await my_workflow.ainvoke(some_input, config)  # Await result asynchronously
    ```
    </Tab>
    <Tab title="Stream">
    ```python
    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    for chunk in my_workflow.stream(some_input, config):
        print(chunk)
    ```
    </Tab>
    <Tab title="Async Stream">
    ```python
    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    async for chunk in my_workflow.astream(some_input, config):
        print(chunk)
    ```
    </Tab>
</Tabs>




### 恢复

在 [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) 之后恢复执行可以通过将**resume**值传递给 [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) 原语来完成。

<Tabs>
    <Tab title="Invoke">
    ```python
    from langgraph.types import Command

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    my_workflow.invoke(Command(resume=some_resume_value), config)
    ```
    </Tab>
    <Tab title="Async Invoke">
    ```python
    from langgraph.types import Command

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    await my_workflow.ainvoke(Command(resume=some_resume_value), config)
    ```
    </Tab>
    <Tab title="Stream">
    ```python
    from langgraph.types import Command

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    for chunk in my_workflow.stream(Command(resume=some_resume_value), config):
        print(chunk)
    ```
    </Tab>
    <Tab title="Async Stream">
    ```python
    from langgraph.types import Command

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    async for chunk in my_workflow.astream(Command(resume=some_resume_value), config):
        print(chunk)
    ```
    </Tab>
</Tabs>




**错误后恢复**

要在错误后恢复，使用 `None` 和相同的**线程 id**（config）运行 `entrypoint`。

这假设底层**错误**已解决，执行可以成功进行。

<Tabs>
    <Tab title="Invoke">
    ```python

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    my_workflow.invoke(None, config)
    ```
    </Tab>
    <Tab title="Async Invoke">
    ```python

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    await my_workflow.ainvoke(None, config)
    ```
    </Tab>
    <Tab title="Stream">
    ```python

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    for chunk in my_workflow.stream(None, config):
        print(chunk)
    ```
    </Tab>
    <Tab title="Async Stream">
    ```python

    config = {
        "configurable": {
            "thread_id": "some_thread_id"
        }
    }

    async for chunk in my_workflow.astream(None, config):
        print(chunk)
    ```
    </Tab>
</Tabs>




### 短期内存

当使用 `checkpointer` 定义 `entrypoint` 时，它会在[检查点](/oss/python/langgraph/persistence#checkpoints)中存储同一**线程 id**上连续调用之间的信息。

这允许使用 `previous` 参数访问来自上一次调用的状态。

默认情况下，`previous` 参数是上一次调用的返回值。

```python
@entrypoint(checkpointer=checkpointer)
def my_workflow(number: int, *, previous: Any = None) -> int:
    previous = previous or 0
    return number + previous

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

my_workflow.invoke(1, config)  # 1 (previous was None)
my_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)
```




#### `entrypoint.final`

[`entrypoint.final`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint.final) 是一个可以从入口点返回的特殊原语，允许**解耦**保存在检查点中的值与入口点的返回值。

第一个值是入口点的返回值，第二个值是将保存在检查点中的值。类型注释是 `entrypoint.final[return_type, save_type]`。

```python
@entrypoint(checkpointer=checkpointer)
def my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:
    previous = previous or 0
    # This will return the previous value to the caller, saving
    # 2 * number to the checkpoint, which will be used in the next invocation
    # for the `previous` parameter.
    return entrypoint.final(value=previous, save=2 * number)

config = {
    "configurable": {
        "thread_id": "1"
    }
}

my_workflow.invoke(3, config)  # 0 (previous was None)
my_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)
```




## 任务

**任务**表示一个离散的工作单元，例如 API 调用或数据处理步骤。它有两个关键特征：

* **异步执行**：任务设计为异步执行，允许多个操作并发运行而不会阻塞。
* **检查点**：任务结果保存到检查点，使工作流能够从最后保存的状态恢复。（有关更多详细信息，请参阅[持久化](/oss/python/langgraph/persistence)）。

### 定义

任务使用 `@task` 装饰器定义，它包装一个常规 Python 函数。

```python
from langgraph.func import task

@task()
def slow_computation(input_value):
    # Simulate a long-running operation
    ...
    return result
```




<Warning>
**序列化**
任务的**输出**必须是 JSON 可序列化的，以支持检查点。
</Warning>

### 执行

**任务**只能从**入口点**、另一个**任务**或[状态图节点](/oss/python/langgraph/graph-api#nodes)内调用。

任务_不能_直接从主应用程序代码调用。

当您调用**任务**时，它会_立即_返回一个 future 对象。Future 是稍后可用结果的占位符。

要获取**任务**的结果，您可以同步等待（使用 `result()`）或异步等待（使用 `await`）。

<Tabs>
    <Tab title="Synchronous Invocation">
    ```python
    @entrypoint(checkpointer=checkpointer)
    def my_workflow(some_input: int) -> int:
        future = slow_computation(some_input)
        return future.result()  # Wait for the result synchronously
    ```
    </Tab>
    <Tab title="Asynchronous Invocation">
    ```python
    @entrypoint(checkpointer=checkpointer)
    async def my_workflow(some_input: int) -> int:
        return await slow_computation(some_input)  # Await result asynchronously
    ```
    </Tab>
</Tabs>




## 何时使用任务

**任务**在以下场景中很有用：

* **检查点**：当您需要将长时间运行的操作结果保存到检查点时，这样在恢复工作流时就不需要重新计算它。
* **人在回路**：如果您正在构建需要人工干预的工作流，您必须使用**任务**来封装任何随机性（例如，API 调用），以确保工作流可以正确恢复。有关更多详细信息，请参阅[确定性](#determinism)部分。
* **并行执行**：对于 I/O 绑定任务，**任务**启用并行执行，允许多个操作并发运行而不会阻塞（例如，调用多个 API）。
* **可观测性**：将操作包装在**任务**中提供了一种跟踪工作流进度并使用 [LangSmith](https://docs.smith.langchain.com/) 监控单个操作执行的方法。
* **可重试工作**：当工作需要重试以处理故障或不一致时，**任务**提供了一种封装和管理重试逻辑的方法。

## 序列化

LangGraph 中的序列化有两个关键方面：

1. `entrypoint` 输入和输出必须是 JSON 可序列化的。
2. `task` 输出必须是 JSON 可序列化的。

这些要求对于启用检查点和工作流恢复是必要的。使用 Python 原语（如字典、列表、字符串、数字和布尔值）来确保您的输入和输出是可序列化的。




序列化确保工作流状态（如任务结果和中间值）可以可靠地保存和恢复。这对于启用人在回路交互、容错和并行执行至关重要。

提供不可序列化的输入或输出将在工作流配置了检查点保存器时导致运行时错误。

## 确定性

要利用**人在回路**等功能，任何随机性都应该封装在**任务**内部。这保证了当执行停止（例如，用于人在回路）然后恢复时，它将遵循相同的_步骤序列_，即使**任务**结果是非确定性的。

LangGraph 通过在**任务**和[**子图**](/oss/python/langgraph/use-subgraphs)执行时持久化它们的结果来实现此行为。设计良好的工作流确保恢复执行遵循_相同的步骤序列_，允许正确检索先前计算的结果，而无需重新执行它们。这对于长时间运行的**任务**或具有非确定性结果的**任务**特别有用，因为它避免了重复先前完成的工作，并允许从本质上相同的地方恢复。

虽然工作流的不同运行可能产生不同的结果，但恢复**特定**运行应始终遵循相同的记录步骤序列。这允许 LangGraph 高效地查找在图被中断之前执行的**任务**和**子图**结果，并避免重新计算它们。

## 幂等性

幂等性确保多次运行同一操作会产生相同的结果。如果由于故障而重新运行步骤，这有助于防止重复的 API 调用和冗余处理。始终将 API 调用放在**任务**函数内以进行检查点，并设计它们以便在重新执行时是幂等的。如果**任务**开始但未成功完成，可能会发生重新执行。然后，如果工作流恢复，**任务**将再次运行。使用幂等键或验证现有结果以避免重复。

## 常见陷阱

### 处理副作用

将副作用（例如，写入文件、发送电子邮件）封装在任务中，以确保在恢复工作流时不会多次执行它们。

<Tabs>
    <Tab title="不正确">
    在此示例中，副作用（写入文件）直接包含在工作流中，因此在恢复工作流时会第二次执行。

    ```python
    @entrypoint(checkpointer=checkpointer)
    def my_workflow(inputs: dict) -> int:
        # This code will be executed a second time when resuming the workflow.
        # Which is likely not what you want.
        with open("output.txt", "w") as f:  # [!code highlight]
            f.write("Side effect executed")  # [!code highlight]
        value = interrupt("question")
        return value
```



    </Tab>
    <Tab title="正确">
    在此示例中，副作用封装在任务中，确保恢复时执行一致。

    ```python
    from langgraph.func import task

    @task  # [!code highlight]
    def write_to_file():  # [!code highlight]
        with open("output.txt", "w") as f:
            f.write("Side effect executed")

    @entrypoint(checkpointer=checkpointer)
    def my_workflow(inputs: dict) -> int:
        # The side effect is now encapsulated in a task.
        write_to_file().result()
        value = interrupt("question")
        return value
```



    </Tab>
</Tabs>

### 非确定性控制流

可能每次给出不同结果的操作（如获取当前时间或随机数）应该封装在任务中，以确保在恢复时返回相同的结果。

* 在任务中：获取随机数 (5) → 中断 → 恢复 → (再次返回 5) → ...
* 不在任务中：获取随机数 (5) → 中断 → 恢复 → 获取新随机数 (7) → ...

在使用具有多个中断调用的**人在回路**工作流时，这一点尤其重要。LangGraph 为每个任务/入口点保留一个恢复值列表。当遇到中断时，它与相应的恢复值匹配。此匹配严格基于**索引**，因此恢复值的顺序应与中断的顺序匹配。




如果在恢复时不保持执行顺序，一个 [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) 调用可能与错误的 `resume` 值匹配，导致不正确的结果。

请阅读[确定性](#determinism)部分以了解更多详细信息。

<Tabs>
    <Tab title="不正确">
    在此示例中，工作流使用当前时间来确定要执行哪个任务。这是非确定性的，因为工作流的结果取决于执行时间。

    ```python
    from langgraph.func import entrypoint

    @entrypoint(checkpointer=checkpointer)
    def my_workflow(inputs: dict) -> int:
        t0 = inputs["t0"]
        t1 = time.time()  # [!code highlight]

        delta_t = t1 - t0

        if delta_t > 1:
            result = slow_task(1).result()
            value = interrupt("question")
        else:
            result = slow_task(2).result()
            value = interrupt("question")

        return {
            "result": result,
            "value": value
        }
```



    </Tab>
    <Tab title="正确">
    在此示例中，工作流使用输入 `t0` 来确定要执行哪个任务。这是确定性的，因为工作流的结果仅取决于输入。

    ```python
    import time

    from langgraph.func import task

    @task  # [!code highlight]
    def get_time() -> float:  # [!code highlight]
        return time.time()

    @entrypoint(checkpointer=checkpointer)
    def my_workflow(inputs: dict) -> int:
        t0 = inputs["t0"]
        t1 = get_time().result()  # [!code highlight]

        delta_t = t1 - t0

        if delta_t > 1:
            result = slow_task(1).result()
            value = interrupt("question")
        else:
            result = slow_task(2).result()
            value = interrupt("question")

        return {
            "result": result,
            "value": value
        }
```



    </Tab>
</Tabs>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\langgraph\functional-api.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
