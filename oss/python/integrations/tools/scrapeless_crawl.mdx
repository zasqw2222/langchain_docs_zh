---
title: Scrapeless Crawl
---

[**Scrapeless**](https://www.scrapeless.com/) offers flexible and feature-rich data acquisition services with extensive parameter customization and multi-format export support. These capabilities empower LangChain to integrate and leverage external data more effectively. The core functional modules include:

**DeepSerp**

- **Google Search**: Enables comprehensive extraction of Google SERP data across all result types.
  - Supports selection of localized Google domains (e.g., `google.com`, `google.ad`) to retrieve region-specific search results.
  - Pagination supported for retrieving results beyond the first page.
  - Supports a search result filtering toggle to control whether to exclude duplicate or similar content.
- **Google Trends**: Retrieves keyword trend data from Google, including popularity over time, regional interest, and related searches.
  - Supports multi-keyword comparison.
  - Supports multiple data types: `interest_over_time`, `interest_by_region`, `related_queries`, and `related_topics`.
  - Allows filtering by specific Google properties (Web, YouTube, News, Shopping) for source-specific trend analysis.

**Universal Scraping**

- Designed for modern, JavaScript-heavy websites, allowing dynamic content extraction.
  - Global premium proxy support for bypassing geo-restrictions and improving reliability.

**Crawler**

- **Crawl**: Recursively crawl a website and its linked pages to extract site-wide content.
  - Supports configurable crawl depth and scoped URL targeting.
- **Scrape**: Extract content from a single webpage with high precision.
  - Supports "main content only" extraction to exclude ads, footers, and other non-essential elements.
  - Allows batch scraping of multiple standalone URLs.

## Overview

### Integration details

| Class | Package | Serializable | JS support |  Version |
| :--- | :--- | :---: | :---: | :---: |
| [ScrapelessCrawlerScrapeTool](https://pypi.org/project/langchain-scrapeless/) | [langchain-scrapeless](https://pypi.org/project/langchain-scrapeless/) | ‚úÖ | ‚ùå |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-scrapeless?style=flat-square&label=%20) |
| [ScrapelessCrawlerCrawlTool](https://pypi.org/project/langchain-scrapeless/) | [langchain-scrapeless](https://pypi.org/project/langchain-scrapeless/) | ‚úÖ | ‚ùå |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-scrapeless?style=flat-square&label=%20) |

### Tool features

|Native async|Returns artifact|Return data|
|:-:|:-:|:-:|
|‚úÖ|‚úÖ|markdown, rawHtml, screenshot@fullPage, json, links, screenshot, html|

## Setup

The integration lives in the `langchain-scrapeless` package.
!pip install langchain-scrapeless

### Credentials

You'll need a Scrapeless API key to use this tool. You can set it as an environment variable:

```python
import os

os.environ["SCRAPELESS_API_KEY"] = "your-api-key"
```

## Instantiation

### ScrapelessCrawlerScrapeTool

The ScrapelessCrawlerScrapeTool allows you to scrape content from one or multiple websites using Scrapeless‚Äôs Crawler Scrape API. You can extract the main content, control formatting, headers, wait times, and output types.

The tool accepts the following parameters:

- `urls` (required, List[str]): One or more URLs of websites you want to scrape.
- `formats` (optional, List[str]): Defines the format(s) of the scraped output. Default is `['markdown']`. Options include:
  - `'markdown'`
  - `'rawHtml'`
  - `'screenshot@fullPage'`
  - `'json'`
  - `'links'`
  - `'screenshot'`
  - `'html'`
- `only_main_content` (optional, bool): Whether to return only the main page content, excluding headers, navs, footers, etc. Default is True.
- `include_tags` (optional, List[str]): A list of HTML tags to include in the output (e.g., `['h1', 'p']`). If set to None, no tags are explicitly included.
- `exclude_tags` (optional, List[str]): A list of HTML tags to exclude from the output. If set to None, no tags are explicitly excluded.
- `headers` (optional, Dict[str, str]): Custom headers to send with the request (e.g., for cookies or user-agent). Default is None.
- `wait_for` (optional, int): Time to wait in milliseconds before scraping. Useful for giving the page time to fully load. Default is `0`.
- `timeout` (optional, int): Request timeout in milliseconds. Default is `30000`.

### ScrapelessCrawlerCrawlTool

The ScrapelessCrawlerCrawlTool allows you to crawl a website starting from a base URL using Scrapeless‚Äôs Crawler Crawl API. It supports advanced filtering of URLs, crawl depth control, content scraping options, headers customization, and more.

The tool accepts the following parameters:

- `url` (required, str): The base URL to start crawling from.

- `limit` (optional, int): Maximum number of pages to crawl. Default is `10000`.
- `include_paths` (optional, List[str]): URL pathname regex patterns to include matching URLs in the crawl. Only URLs matching these patterns will be included. For example, setting `["blog/.*"]` will only include URLs under the `/blog/` path. Default is None.
- `exclude_paths` (optional, List[str]): URL pathname regex patterns to exclude matching URLs from the crawl. For example, setting `["blog/.*"]` will exclude URLs under the `/blog/` path. Default is None.
- `max_depth` (optional, int): Maximum crawl depth relative to the base URL, measured by the number of slashes in the URL path. Default is `10`.
- `max_discovery_depth` (optional, int): Maximum crawl depth based on discovery order. Root and sitemapped pages have depth `0`. For example, setting to `1` and ignoring sitemap will crawl only the entered URL and its immediate links. Default is None.
- `ignore_sitemap` (optional, bool): Whether to ignore the website sitemap during crawling. Default is False.
- `ignore_query_params` (optional, bool): Whether to ignore query parameter differences to avoid re-scraping similar URLs. Default is False.
- `deduplicate_similar_urls` (optional, bool): Whether to deduplicate similar URLs. Default is True.
- `regex_on_full_url` (optional, bool): Whether regex matching applies to the full URL instead of just the path. Default is True.
- `allow_backward_links` (optional, bool): Whether to allow crawling backlinks outside the URL hierarchy. Default is False.
- `allow_external_links` (optional, bool): Whether to allow crawling links to external websites. Default is False.
- `delay` (optional, int): Delay in seconds between page scrapes to respect rate limits. Default is `1`.
- `formats` (optional, List[str]): The format(s) of the scraped content. Default is ["markdown"]. Options include:
  - `'markdown'`
  - `'rawHtml'`
  - `'screenshot@fullPage'`
  - `'json'`
  - `'links'`
  - `'screenshot'`
  - `'html'`
- `only_main_content` (optional, bool): Whether to return only the main content, excluding headers, navigation bars, footers, etc. Default is True.
- `include_tags` (optional, List[str]): List of HTML tags to include in the output (e.g., `['h1', 'p']`). Default is None (no explicit include filter).
- `exclude_tags` (optional, List[str]): List of HTML tags to exclude from the output. Default is None (no explicit exclude filter).
- `headers` (optional, Dict[str, str]): Custom HTTP headers to send with the requests, such as cookies or user-agent strings. Default is None.
- `wait_for` (optional, int): Time in milliseconds to wait before scraping the content, allowing the page to load fully. Default is `0`.
- `timeout` (optional, int):Request timeout in milliseconds. Default is `30000`.

## Invocation

### ScrapelessCrawlerCrawlTool

#### Usage with Parameters

```python
from langchain_scrapeless import ScrapelessCrawlerCrawlTool

tool = ScrapelessCrawlerCrawlTool()

# Advanced usage
result = tool.invoke({"url": "https://exmaple.com", "limit": 4})
print(result)
```

```output
{'success': True, 'status': 'completed', 'completed': 1, 'total': 1, 'data': [{'markdown': '# Well hello there.\n\nWelcome to exmaple.com.\n\nChances are you got here by mistake (example.com, anyone?)', 'metadata': {'scrapeId': '547b2478-a41a-4a17-8015-8db378ee455f', 'sourceURL': 'https://exmaple.com', 'url': 'https://exmaple.com', 'statusCode': 200}}]}
```

#### Use within an agent

```python
from langchain_openai import ChatOpenAI
from langchain_scrapeless import ScrapelessCrawlerCrawlTool
from langchain.agents import create_agent


model = ChatOpenAI()

tool = ScrapelessCrawlerCrawlTool()

# Use the tool with an agent
tools = [tool]
agent = create_agent(model, tools)

for chunk in agent.stream(
    {
        "messages": [
            (
                "human",
                "Use the scrapeless crawler crawl tool to crawl the website https://example.com and output the markdown content as a string.",
            )
        ]
    },
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()
```

```output
================================ Human Message =================================

Use the scrapeless crawler crawl tool to crawl the website https://example.com and output the markdown content as a string.
================================== Ai Message ==================================
Tool Calls:
  scrapeless_crawler_crawl (call_Ne5HbxqsYDOKFaGDSuc4xppB)
 Call ID: call_Ne5HbxqsYDOKFaGDSuc4xppB
  Args:
    url: https://example.com
    formats: ['markdown']
    limit: 1
================================= Tool Message =================================
Name: scrapeless_crawler_crawl

{"success": true, "status": "completed", "completed": 1, "total": 1, "data": [{"markdown": "# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)", "metadata": {"viewport": "width=device-width, initial-scale=1", "title": "Example Domain", "scrapeId": "00561460-9166-492b-8fed-889667383e55", "sourceURL": "https://example.com", "url": "https://example.com", "statusCode": 200}}]}
================================== Ai Message ==================================

The crawl of the website https://example.com has been completed. Here is the markdown content extracted from the website:

\`\`\`
# Example Domain

This domain is for use in illustrative examples in documents. You may use this
domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)
\`\`\`

You can find more information on the website [here](https://www.iana.org/domains/example).
```

### ScrapelessCrawlerScrapeTool

#### Usage with Parameters

```python
from langchain_scrapeless import ScrapelessDeepSerpGoogleTrendsTool

tool = ScrapelessDeepSerpGoogleTrendsTool()

# Basic usage
result = tool.invoke("Funny 2048,negamon monster trainer")
print(result)
```

```output
{'parameters': {'engine': 'google.trends.search', 'hl': 'en', 'data_type': 'INTEREST_OVER_TIME', 'tz': '0', 'cat': '0', 'date': 'today 1-m', 'q': 'Funny 2048,negamon monster trainer'}, 'interest_over_time': {'timeline_data': [{'date': 'Jul 11, 2025', 'timestamp': '1752192000', 'value': [0, 0]}, {'date': 'Jul 12, 2025', 'timestamp': '1752278400', 'value': [0, 0]}, {'date': 'Jul 13, 2025', 'timestamp': '1752364800', 'value': [0, 0]}, {'date': 'Jul 14, 2025', 'timestamp': '1752451200', 'value': [0, 0]}, {'date': 'Jul 15, 2025', 'timestamp': '1752537600', 'value': [0, 0]}, {'date': 'Jul 16, 2025', 'timestamp': '1752624000', 'value': [0, 0]}, {'date': 'Jul 17, 2025', 'timestamp': '1752710400', 'value': [0, 0]}, {'date': 'Jul 18, 2025', 'timestamp': '1752796800', 'value': [0, 0]}, {'date': 'Jul 19, 2025', 'timestamp': '1752883200', 'value': [0, 0]}, {'date': 'Jul 20, 2025', 'timestamp': '1752969600', 'value': [0, 0]}, {'date': 'Jul 21, 2025', 'timestamp': '1753056000', 'value': [0, 0]}, {'date': 'Jul 22, 2025', 'timestamp': '1753142400', 'value': [0, 0]}, {'date': 'Jul 23, 2025', 'timestamp': '1753228800', 'value': [0, 0]}, {'date': 'Jul 24, 2025', 'timestamp': '1753315200', 'value': [0, 0]}, {'date': 'Jul 25, 2025', 'timestamp': '1753401600', 'value': [0, 0]}, {'date': 'Jul 26, 2025', 'timestamp': '1753488000', 'value': [0, 0]}, {'date': 'Jul 27, 2025', 'timestamp': '1753574400', 'value': [0, 0]}, {'date': 'Jul 28, 2025', 'timestamp': '1753660800', 'value': [0, 0]}, {'date': 'Jul 29, 2025', 'timestamp': '1753747200', 'value': [0, 0]}, {'date': 'Jul 30, 2025', 'timestamp': '1753833600', 'value': [0, 0]}, {'date': 'Jul 31, 2025', 'timestamp': '1753920000', 'value': [0, 0]}, {'date': 'Aug 1, 2025', 'timestamp': '1754006400', 'value': [0, 0]}, {'date': 'Aug 2, 2025', 'timestamp': '1754092800', 'value': [0, 0]}, {'date': 'Aug 3, 2025', 'timestamp': '1754179200', 'value': [0, 0]}, {'date': 'Aug 4, 2025', 'timestamp': '1754265600', 'value': [0, 0]}, {'date': 'Aug 5, 2025', 'timestamp': '1754352000', 'value': [0, 0]}, {'date': 'Aug 6, 2025', 'timestamp': '1754438400', 'value': [0, 0]}, {'date': 'Aug 7, 2025', 'timestamp': '1754524800', 'value': [0, 0]}, {'date': 'Aug 8, 2025', 'timestamp': '1754611200', 'value': [0, 0]}, {'date': 'Aug 9, 2025', 'timestamp': '1754697600', 'value': [0, 0]}, {'date': 'Aug 10, 2025', 'timestamp': '1754784000', 'value': [0, 100]}, {'date': 'Aug 11, 2025', 'timestamp': '1754870400', 'value': [0, 0]}], 'averages': [{'value': 0}, {'value': 3}], 'isPartial': True}}
```

#### Advanced Usage with Parameters

```python
from langchain_scrapeless import ScrapelessCrawlerScrapeTool

tool = ScrapelessCrawlerScrapeTool()

result = tool.invoke(
    {
        "urls": ["https://exmaple.com", "https://www.scrapeless.com/en"],
        "formats": ["markdown"],
    }
)
print(result)
```

```output
{'success': True, 'status': 'completed', 'completed': 1, 'total': 1, 'data': [{'markdown': "[ü©µ Don't just take our word for it. See what our users say on Product Hunt.](https://www.producthunt.com/posts/scrapeless-deep-serpapi)\n\n# Effortless Web Scraping Toolkit  for Business and Developers\n\nThe ultimate scraper's companion: an expandable suite of tools, including\n\nScraping Browser, Scraping API, Universal Scraping API\n\nand Anti-Bot Solutions‚Äîdesigned to work together or independently.\n\n[**4.8**](https://www.g2.com/products/scrapeless/reviews) [**4.5**](https://www.trustpilot.com/review/scrapeless.com) [**4.8**](https://slashdot.org/software/p/Scrapeless/) [**8.5**](https://tekpon.com/software/scrapeless/reviews/)\n\nNo credit card required\n\n## A Flexible Toolkit for Accessing Public Web Data\n\nAI-powered seamless data extraction, effortlessly bypassing blocks with a single API call.\n\n[scrapeless](https://www.scrapeless.com/en)\n\n[![Deep SerpApi](https://www.scrapeless.com/_next/image?url=%2Fassets%2Fimages%2Ftoolkit%2Flight%2Fimg-2.png&w=750&q=100)\\\\\n\\\\\nView more\\\\\n\\\\\n20+ custom parameters\\\\\n\\\\\n20+ Google SERP scenarios\\\\\n\\\\\nPrecision Search Fueling LLM & RAG AI\\\\\n\\\\\n1-2s response; $0.1/1k queries](https://www.scrapeless.com/en/product/deep-serp-api) [![Scraping Browser](https://www.scrapeless.com/_next/image?url=%2Fassets%2Fimages%2Ftoolkit%2Flight%2Fimg-4.png&w=750&q=100)\\\\\n\\\\\nView more\\\\\n\\\\\nHuman-like Behavior\\\\\n\\\\\nHigh Performance\\\\\n\\\\\nBypassing Risk Control\\\\\n\\\\\nConnect using the CDP Protocol](https://www.scrapeless.com/en/product/scraping-browser) [![Universal Scraping API](https://www.scrapeless.com/_next/image?url=%2Fassets%2Fimages%2Ftoolkit%2Flight%2Fimg-1.png&w=750&q=100)\\\\\n\\\\\nView more\\\\\n\\\\\nSession Mode\\\\\n\\\\\nCustom TLS\\\\\n\\\\\nJs Render](https://www.scrapeless.com/en/product/universal-scraping-api)\n\n### Customized Services\n\nContact our technical experts for custom solutions.\n\nBook a demo\n\n## From Simple Data Scraping to Complex Anti-Bot Challenges,   Scrapeless Has You Covered.\n\nFlexible Toolkit for Adapting to Diverse Data Extraction Needs.\n\n[Try for Free](https://app.scrapeless.com/passport/register)\n\n### Fully Compatible with Key Programming Languages and Tools\n\nSeamlessly integrate across all devices, OS, and languages. Worry-free compatibility ensures smooth data collection.\n\nGet all example codes on the dashboard after login\n\n![scrapeless](https://www.scrapeless.com/_next/image?url=%2Fassets%2Fimages%2Fcode%2Fcode-l.jpg&w=3840&q=75)\n\n## Enterprise-level Data Scraping Solution\n\nHigh-quality, tailored web scraping solutions and expert services designed for critical business projects.\n\n### Customized Data Scraping Solutions\n\nTailored web scraping services designed to address your\xa0 unique business requirements and deliver actionable insights.\n\n### High Concurrency and High-Performance Scraping\n\nEfficiently gather massive volumes of data with unparalleled speed and reliability,\xa0ensuring optimal performance even under heavy load.\n\n### Data Cleaning and Transformation\n\nEnhance data accuracy and usability through comprehensive\xa0 cleaning and transformation processes, turning raw data into\xa0 valuable information.\n\n### Real-Time Data Push and API Integration\n\nSeamlessly integrate and access live data streams with robust APIs,\xa0ensuring your applications are always up-to-date with the latest information.\n\n### Data Security and Privacy Protection\n\nProtect your data with state-of-the-art security measures and strict\xa0compliance standards, ensuring privacy and confidentiality at every step.\n\n### Enterprise-level SLA\n\nThe Service Level Agreement (SLA) serves as a safeguard for your project,\xa0ensuring a contract for anticipated outcomes, automated oversight, prompt issue\xa0resolution, and a personalized maintenance plan.\n\n## Why Scrapeless: Simplify Your Data Flow Effortlessly.\n\nAchieve all your data scraping tasks with more power, simplicity, and cost-effectiveness in less time.\n\n### Articles\n\nNews articles/Blog posts/Research papers\n\n### Organized Fresh Data\n\n### Prices\n\nProduct prices/Discount information/Market trend analysis\n\n### No need to hassle with browser maintenance\n\n### Reviews\n\nProduct reviews/User feedback/Social media reviews\n\n### Only pay for successful requests\n\n### Products\n\nProduct Launches/Tech Specs/Product Comparisons\n\n### Fully scalable\n\n## Unleash Your Competitive Edge  in Data within the Industry\n\n## Regulate Compliance for All Users\n\nContact us\n\nWe are committed to using technology for the benefit of humanity and firmly oppose any illegal activities and misuse of our products. We support the collection of publicly available data to improve human life, while strongly opposing the collection of unauthorized or unapproved sensitive information. If you find anyone abusing our services, please provide us with feedback! To further enhance user confidence and control, we have established a dedicated Privacy Center aimed at empowering users with more capabilities and information rights.\n\n![scrapeless](https://www.scrapeless.com/_next/image?url=%2Fassets%2Fimages%2Fregulate-compliance.png&w=640&q=75)\n\n## Web Scraping Blog\n\nMost comprehensive guide, created for all Web Scraping developers.\n\n[View All Blogs](https://www.scrapeless.com/en/blog)\n\n[**Scrapeless MCP Server Is Officially Live! Build Your Ultimate AI-Web Connector** \\\\\n\\\\\nDiscover how the Scrapeless MCP Server gives LLMs real-time web browsing and scraping abilities. Learn how to build AI agents that search, extract, and interact with dynamic web content seamlessly.\\\\\n\\\\\n![Michael Lee](https://www.scrapeless.com/_next/image?url=https%3A%2F%2Fassets.scrapeless.com%2Fprod%2Fimages%2Fauthor-avatars%2Fmichael-lee.png&w=48&q=75)Michael Lee\\\\\n\\\\\n17-Jul-2025\\\\\n\\\\\n![Scrapeless MCP Server](https://www.scrapeless.com/_next/image?url=https%3A%2F%2Fassets.scrapeless.com%2Fprod%2Fposts%2Fscrapeless-mcp-server%2Fc85738fc1c504abe930fd4514e4a2190.jpeg&w=3840&q=75)](https://www.scrapeless.com/en/blog/scrapeless-mcp-server) [**Product Updates \\| New Profile Feature** \\\\\n\\\\\nProduct Updates \\| Introducing the new Profile feature to enable persistent browser data storage, streamline cross-session workflows, and boost automation efficiency.\\\\\n\\\\\n![Emily Chen](https://www.scrapeless.com/_next/image?url=https%3A%2F%2Fassets.scrapeless.com%2Fprod%2Fimages%2Fauthor-avatars%2Femily-chen.png&w=48&q=75)Emily Chen\\\\\n\\\\\n17-Jul-2025\\\\\n\\\\\n![Product Updates | New Profile Feature: Make Browser Data Persistent, Efficient, and Controllable](https://www.scrapeless.com/_next/image?url=https%3A%2F%2Fassets.scrapeless.com%2Fprod%2Fposts%2Fscrapeelss-profile%2F3194244c16c9b56e1592640ea95c389e.jpeg&w=3840&q=75)](https://www.scrapeless.com/en/blog/scrapeelss-profile) [**How to Track Your Ranking on ChatGPT?** \\\\\n\\\\\nLearn why traditional SEO tools fall short and how Scrapeless helps you monitor and optimize your AI rankings effortlessly.\\\\\n\\\\\n![Michael Lee](https://www.scrapeless.com/_next/image?url=https%3A%2F%2Fassets.scrapeless.com%2Fprod%2Fimages%2Fauthor-avatars%2Fmichael-lee.png&w=48&q=75)Michael Lee\\\\\n\\\\\n01-Jul-2025\\\\\n\\\\\n![ChatGPT Scraper](https://www.scrapeless.com/_next/image?url=https%3A%2F%2Fassets.scrapeless.com%2Fprod%2Fposts%2Fchatgpt-scraper%2F7c5b1ac494b6838a7eca2964df15ef59.png&w=3840&q=75)](https://www.scrapeless.com/en/blog/chatgpt-scraper)\n\nContact our sales team\n\nMonday to Friday, 9:00 AM - 18:00 PMSingapore Standard Time (UTC+08:00)\n\nScrapeless offers AI-powered, robust, and scalable web scraping and automation services trusted by leading enterprises. Our enterprise-grade solutions are tailored to meet your project needs, with dedicated technical support throughout. With a strong technical team and flexible delivery times, we charge only for successful data, enabling efficient data extraction while bypassing limitations.\n\nContact us now to fuel your business growth.\n\n[**4.8**](https://www.g2.com/products/scrapeless/reviews) [**4.5**](https://www.trustpilot.com/review/scrapeless.com) [**4.8**](https://slashdot.org/software/p/Scrapeless/) [**8.5**](https://tekpon.com/software/scrapeless/reviews/)\n\nBook a demo\n\nProvide your contact details, and we'll promptly reach out to offer a product demo and introduction. We ensure your information remains confidential, complying with GDPR standards.\n\nGet a demo\n\nRegister and Claim Free Trial\n\nYour free trial is ready! Sign up for a Scrapeless account for free, and your trial will be instantly activated in your account.\n\n[Sign up](https://app.scrapeless.com/passport/register)\n\nWe value your privacy\n\nWe use cookies to analyze website usage and do not record any of your personal information. View [Privacy Policy](https://www.scrapeless.com/en/legal/privacy-policy)\n\nReject\n\nAccept", 'metadata': {'language': 'en', 'description': 'Scrapeless is the best full-stack web scraping toolkit offering Scraping API, Scraping Browser, Universal Scraping API, Captcha Solver, and Proxies, designed to handle all your data collection needs with ease and reliability, empowering businesses and developers with efficient data extraction solutions.', 'google-site-verification': 'xj1xDpU8LpGG_h-2lIBVW_6GNW5Vtx0h5M3lz43HUXc', 'viewport': 'width=device-width, initial-scale=1', 'keywords': 'Scraping API, Scraping Browser, Universal Scraping API, Captcha Solver, and Proxies, web scraping,  web scraper, web scraping api, Web scraper,data scraping, web crawler', 'next-size-adjust': '', 'favicon': 'https://www.scrapeless.com/favicon.ico', 'title': 'Effortless Web Scraping Toolkit - Scrapeless', 'scrapeId': 'c7189211-7034-4e86-9afd-89fa5268b013', 'sourceURL': 'https://www.scrapeless.com/en', 'url': 'https://www.scrapeless.com/en', 'statusCode': 200}}]}
```

#### Use within an agent

```python
from langchain_openai import ChatOpenAI
from langchain_scrapeless import ScrapelessCrawlerScrapeTool
from langchain.agents import create_agent


model = ChatOpenAI()

tool = ScrapelessCrawlerScrapeTool()

# Use the tool with an agent
tools = [tool]
agent = create_agent(model, tools)

for chunk in agent.stream(
    {
        "messages": [
            (
                "human",
                "Use the scrapeless crawler scrape tool to get the website content of https://example.com and output the html content as a string.",
            )
        ]
    },
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()
```

```output
================================ Human Message =================================

Use the scrapeless crawler scrape tool to get the website content of https://example.com and output the html content as a string.
================================== Ai Message ==================================
Tool Calls:
  scrapeless_crawler_scrape (call_qrPMGLjXmzb5QlVoIZgMuyPN)
 Call ID: call_qrPMGLjXmzb5QlVoIZgMuyPN
  Args:
    urls: ['https://example.com']
    formats: ['html']
================================= Tool Message =================================
Name: scrapeless_crawler_scrape

{"success": true, "status": "completed", "completed": 1, "total": 1, "data": [{"metadata": {"viewport": "width=device-width, initial-scale=1", "title": "Example Domain", "scrapeId": "63070ee5-ebef-4727-afe7-2b06466c6777", "sourceURL": "https://example.com", "url": "https://example.com", "statusCode": 200}, "html": "<!DOCTYPE html><html>\n\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n</div>\n\n\n<div id=\"div-f3t6fv31hyl\" style=\"display: none;\"></div></body></html>"}]}
================================== Ai Message ==================================

The HTML content of the website "https://example.com" is as follows:

\`\`\`html
<!DOCTYPE html><html>
<body>
<div>
    <h1>Example Domain</h1>
    <p>This domain is for use in illustrative examples in documents. You may use this
    domain in literature without prior coordination or asking for permission.</p>
    <p><a href="https://www.iana.org/domains/example">More information...</a></p>
</div>

<div id="div-f3t6fv31hyl" style="display: none;"></div></body></html>
\`\`\`
```

---

## API reference

- [Scrapeless Documentation](https://docs.scrapeless.com/en/crawl/quickstart/introduction/)
- [Scrapeless API Reference](https://apidocs.scrapeless.com/api-17509003)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\python\integrations\tools\scrapeless_crawl.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
