---
title: Prolog
---

LangChain tools that use Prolog rules to generate answers.

## Overview

The PrologTool class allows the generation of langchain tools that use Prolog rules to generate answers.

## Setup

Let's use the following Prolog rules in the file family.pl:

parent(john, bianca, mary).\
parent(john, bianca, michael).\
parent(peter, patricia, jennifer).\
partner(X, Y) :- parent(X, Y, _).

```python
#!pip install langchain-prolog

from langchain_prolog import PrologConfig, PrologRunnable, PrologTool

TEST_SCRIPT = "family.pl"
```

## Instantiation

First create the Prolog tool:

```python
schema = PrologRunnable.create_schema("parent", ["men", "women", "child"])
config = PrologConfig(
    rules_file=TEST_SCRIPT,
    query_schema=schema,
)
prolog_tool = PrologTool(
    prolog_config=config,
    name="family_query",
    description="""
        Query family relationships using Prolog.
        parent(X, Y, Z) implies only that Z is a child of X and Y.
        Input can be a query string like 'parent(john, X, Y)' or 'john, X, Y'"
        You have to specify 3 parameters: men, woman, child. Do not use quotes.
    """,
)
```

## Invocation

### Using a Prolog tool with an LLM and function calling

```python
#!pip install python-dotenv

from dotenv import find_dotenv, load_dotenv

load_dotenv(find_dotenv(), override=True)

#!pip install langchain-openai

from langchain.messages import HumanMessage
from langchain_openai import ChatOpenAI
```

To use the tool, bind it to the LLM model:

```python
model = ChatOpenAI(model="gpt-4o-mini")
model_with_tools = model.bind_tools([prolog_tool])
```

and then query the model:

```python
query = "Who are John's children?"
messages = [HumanMessage(query)]
response = model_with_tools.invoke(messages)
```

The LLM will respond with a tool call request:

```python
messages.append(response)
response.tool_calls[0]
```

```output
{'name': 'family_query',
 'args': {'men': 'john', 'women': None, 'child': None},
 'id': 'call_gH8rWamYXITrkfvRP2s5pkbF',
 'type': 'tool_call'}
```

The tool takes this request and queries the Prolog database:

```python
tool_msg = prolog_tool.invoke(response.tool_calls[0])
```

The tool returns a list with all the solutions for the query:

```python
messages.append(tool_msg)
tool_msg
```

```output
ToolMessage(content='[{"Women": "bianca", "Child": "mary"}, {"Women": "bianca", "Child": "michael"}]', name='family_query', tool_call_id='call_gH8rWamYXITrkfvRP2s5pkbF')
```

That we then pass to the LLM, and the LLM answers the original query using the tool response:

```python
answer = model_with_tools.invoke(messages)
print(answer.content)
```

```output
John has two children: Mary and Michael, with Bianca as their mother.
```

## Chaining

### Using a Prolog Tool with an agent

To use the prolog tool with an agent, pass it to the agent's constructor:

```python
#!pip install langgraph

from langchain.agents import create_agent


agent_executor = create_agent(model, [prolog_tool])
```

The agent takes the query and use the Prolog tool if needed:

```python
messages = agent_executor.invoke({"messages": [("human", query)]})
```

Then the agent receives the tool response and generates the answer:

```python
messages["messages"][-1].pretty_print()
```

```output
================================== Ai Message ==================================

John has two children: Mary and Michael, with Bianca as their mother.
```

---

## API reference

See [langchain-prolog.readthedocs.io/en/latest/modules.html](https://langchain-prolog.readthedocs.io/en/latest/modules.html) for detail.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\python\integrations\tools\prolog_tool.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
