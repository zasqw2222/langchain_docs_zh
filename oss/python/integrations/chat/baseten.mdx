---
title: ChatBaseten
---

This guide provides a quick overview for getting started with the Baseten [chat model](/oss/python/langchain/models). For a detailed listing of all ChatBaseten features, parameters, and configurations, head to the [ChatBaseten API reference](https://python.langchain.com/api_reference/baseten/chat_models/langchain_baseten.chat_models.ChatBaseten.html).

Baseten provides inference designed for production applications. Built on the Baseten Inference Stack, these APIs deliver enterprise-grade performance and reliability for leading open-source or custom models: https://www.baseten.co/library/.

## Overview

### Details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatBaseten](https://python.langchain.com/api_reference/baseten/chat_models/langchain_baseten.chat_models.ChatBaseten.html) | [langchain-baseten](https://python.langchain.com/api_reference/baseten/index.html) | ❌ | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-baseten?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-baseten?style=flat-square&label=%20) |

### Features

| [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming/) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ❌ |

Model APIs only support text input, while some dedicated deployments support image and audio input depending on model. Check the Baseten model library for details: https://www.baseten.co/library/

---

## Setup

To access Baseten models, you'll need to create a Baseten account, get an API key, and install the `langchain-baseten` integration package.

Head to [this page](https://app.baseten.co) to create an account with Baseten and generate an API key. Once you've done this, set the BASETEN_API_KEY environment variable:

### Credentials

```python Set API key icon="key"
import getpass
import os

if "BASETEN_API_KEY" not in os.environ:
    os.environ["BASETEN_API_KEY"] = getpass.getpass("Enter your Baseten API key: ")
```

To enable automated <Tooltip tip="Log each step of a model's execution to debug and improve it">tracing</Tooltip> of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:

```python Enable tracing icon="flask"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"
```

### Installation

The LangChain Baseten integration lives in the `langchain-baseten` package:

<CodeGroup>
    ```python pip
    pip install -U langchain-baseten
    ```
    ```python uv
    uv add langchain-baseten
    ```
</CodeGroup>

---

## Instantiation

Baseten offers two ways to access chat models:

1. **Model APIs**: For access to the latest, most popular opensource models.
2. **Dedicated URLs**: Use specific model deployments with dedicated resources.

Both approaches are supported with automatic endpoint normalization.


```python Initialize with model slug icon="robot"
from langchain_baseten import ChatBaseten

# Option 1: Use Model APIs with model slug
model = ChatBaseten(
    model="moonshotai/Kimi-K2-Instruct-0905",  # Choose from available model slugs: https://docs.baseten.co/development/model-apis/overview#supported-models
    api_key="your-api-key",  # Or set BASETEN_API_KEY env var
)
```

```python Initialize with model URL icon="link"
from langchain_baseten import ChatBaseten

# Option 2: Use dedicated deployments with model url
model = ChatBaseten(
    model_url="https://model-<id>.api.baseten.co/environments/production/predict",
    api_key="your-api-key",  # Or set BASETEN_API_KEY env var
)
```

---

## Invocation

```python Basic invocation icon="play"
# Use the chat model
response = model.invoke("Hello, how are you?")
print(response.content)
```

```output
content="Hello! I'm doing well, thank you for asking! How about you?" additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run--908651ec-00d7-4992-a320-864397c14e37-0'
```

You can also use message objects for more complex conversations:

<CodeGroup>
    ```python Dictionary format icon="book"
    messages = [
        {"role": "system", "content": "You are a poetry expert"},
        {"role": "user", "content": "Write a haiku about spring"},
    ]
    response = model.invoke(messages)
    print(response)
    ```
</CodeGroup>
```output
content='Buds yawn open wide—  \na robin stitches the hush  \nwith threads of first light.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run--6f7d1db7-daae-4628-a40a-2ab7323e8f15-0'
```


<Tip>
    Full guides are available on [chat model invocation types](/oss/python/langchain/models#invocation), [message types](/oss/python/langchain/messages#message-types), and [content blocks](/oss/python/langchain/messages#standard-content-blocks).
</Tip>

---

## API reference

For detailed documentation of all ChatBaseten features and configurations, head to the [API reference](https://python.langchain.com/api_reference/baseten/chat_models/langchain_baseten.chat_models.ChatBaseten.html).

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\python\integrations\chat\baseten.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
