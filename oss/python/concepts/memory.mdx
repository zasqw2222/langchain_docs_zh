---
title: 内存概述
sidebarTitle: 内存
---



[内存](/oss/python/langgraph/add-memory)是一个能够记住先前交互信息的系统。对于 AI 智能体来说,内存至关重要,因为它让智能体能够记住先前的交互、从反馈中学习并适应用户偏好。随着智能体处理更复杂的任务和更多的用户交互,这种能力对于效率和用户满意度都变得至关重要。

本概念指南涵盖了两种基于召回范围的内存类型:

* [短期内存](#short-term-memory),或[线程](/oss/python/langgraph/persistence#threads)范围的内存,通过在会话中维护消息历史来跟踪正在进行的对话。LangGraph 将短期内存作为智能体[状态](/oss/python/langgraph/graph-api#state)的一部分进行管理。状态通过[检查点](/oss/python/langgraph/persistence#checkpoints)持久化到数据库,以便线程可以在任何时候恢复。短期内存在图被调用或步骤完成时更新,并在每个步骤开始时读取状态。
* [长期内存](#long-term-memory)跨会话存储用户特定或应用程序级别的数据,并在对话线程_之间_共享。它可以在_任何时间_和_任何线程_中被召回。内存的范围是任何自定义命名空间,而不仅仅是单个线程 ID。LangGraph 提供[存储](/oss/python/langgraph/persistence#memory-store)([参考文档](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore))让您保存和召回长期记忆。

![](/oss/images/short-vs-long.png)

## 短期内存

[短期内存](/oss/python/langgraph/add-memory#add-short-term-memory)让您的应用程序能够记住单个[线程](/oss/python/langgraph/persistence#threads)或对话中的先前交互。[线程](/oss/python/langgraph/persistence#threads)在会话中组织多个交互,类似于电子邮件在单个对话中对消息进行分组的方式。

LangGraph 将短期内存作为智能体状态的一部分进行管理,通过线程范围的检查点进行持久化。此状态通常可以包括对话历史以及其他有状态数据,例如上传的文件、检索的文档或生成的工件。通过将这些存储在图的状态中,机器人可以访问给定对话的完整上下文,同时保持不同线程之间的分离。

### 管理短期内存

对话历史是最常见的短期内存形式,而长对话对当今的 LLM 构成了挑战。完整的历史可能无法适应 LLM 的上下文窗口,导致无法恢复的错误。即使您的 LLM 支持完整的上下文长度,大多数 LLM 在长上下文中的表现仍然很差。它们会被过时或离题的内容"分散注意力",同时遭受更慢的响应时间和更高的成本。

聊天模型使用消息接受上下文,其中包括开发人员提供的指令(系统消息)和用户输入(人类消息)。在聊天应用程序中,消息在人类输入和模型响应之间交替,导致消息列表随时间变长。由于上下文窗口有限且富含令牌的消息列表可能成本高昂,许多应用程序可以从使用手动删除或遗忘过时信息的技术中受益。

![](/oss/images/filter.png)

有关管理消息的常见技术的更多信息,请参阅[添加和管理内存](/oss/python/langgraph/add-memory#manage-short-term-memory)指南。

## 长期内存

LangGraph 中的[长期内存](/oss/python/langgraph/add-memory#add-long-term-memory)允许系统在不同的对话或会话中保留信息。与**线程范围**的短期内存不同,长期内存保存在自定义"命名空间"中。

长期内存是一个复杂的挑战,没有一刀切的解决方案。然而,以下问题提供了一个框架来帮助您导航不同的技术:

* 内存的类型是什么?人类使用记忆来记住事实([语义记忆](#semantic-memory))、经历([情景记忆](#episodic-memory))和规则([程序性记忆](#procedural-memory))。AI 智能体可以以相同的方式使用内存。例如,AI 智能体可以使用内存来记住有关用户的特定事实以完成任务。
* [何时更新记忆?](#writing-memories)内存可以作为智能体应用程序逻辑的一部分进行更新(例如,"在热路径上")。在这种情况下,智能体通常在响应用户之前决定记住事实。或者,内存可以作为后台任务更新(在后台/异步运行并生成记忆的逻辑)。我们在[下面的部分](#writing-memories)中解释了这些方法之间的权衡。

不同的应用程序需要各种类型的内存。虽然这个类比并不完美,但检查[人类记忆类型](https://www.psychologytoday.com/us/basics/memory/types-of-memory?ref=blog.langchain.dev)可能会有所启发。一些研究(例如,[CoALA 论文](https://arxiv.org/pdf/2309.02427))甚至将这些人类记忆类型映射到 AI 智能体中使用的类型。

| 内存类型 | 存储内容 | 人类示例 | 智能体示例 |
|-------------|----------------|---------------|---------------|
| [语义](#semantic-memory) | 事实 | 我在学校学到的东西 | 关于用户的事实 |
| [情景](#episodic-memory) | 经历 | 我做过的事情 | 过去的智能体行动 |
| [程序性](#procedural-memory) | 指令 | 本能或运动技能 | 智能体系统提示 |

### 语义记忆

[语义记忆](https://en.wikipedia.org/wiki/Semantic_memory),无论是在人类还是 AI 智能体中,都涉及特定事实和概念的保留。在人类中,它可以包括在学校学到的信息以及对概念及其关系的理解。对于 AI 智能体,语义记忆通常用于通过记住过去交互中的事实或概念来个性化应用程序。

<Note>
语义记忆不同于"语义搜索",后者是一种使用"含义"(通常作为嵌入)查找相似内容的技术。语义记忆是心理学中的一个术语,指的是存储事实和知识,而语义搜索是一种基于含义而不是精确匹配来检索信息的方法。
</Note>

语义记忆可以通过不同的方式进行管理:

#### 配置文件

记忆可以是一个单一的、持续更新的"配置文件",包含关于用户、组织或其他实体(包括智能体本身)的范围明确和具体的信息。配置文件通常只是一个 JSON 文档,包含您选择用来表示您的领域的各种键值对。

在记住配置文件时,您需要确保每次都在**更新**配置文件。因此,您需要传入先前的配置文件并[要求模型生成新的配置文件](https://github.com/langchain-ai/memory-template)(或一些[JSON 补丁](https://github.com/hinthornw/trustcall)应用于旧配置文件)。随着配置文件变大,这可能会变得容易出错,并且可能受益于将配置文件拆分为多个文档或在生成文档时使用**严格**解码以确保内存模式保持有效。

![](/oss/images/update-profile.png)

#### 集合

或者,记忆可以是随时间持续更新和扩展的文档集合。每个单独的记忆可以更窄范围且更容易生成,这意味着您不太可能随时间**丢失**信息。对于 LLM 来说,为新信息生成_新_对象比将新信息与现有配置文件协调更容易。因此,文档集合往往会导致[下游更高的召回率](https://en.wikipedia.org/wiki/Precision_and_recall)。

然而,这将一些复杂性转移到了内存更新上。模型现在必须_删除_或_更新_列表中的现有项目,这可能很棘手。此外,一些模型可能默认过度插入,而其他模型可能默认过度更新。请参阅 [Trustcall](https://github.com/hinthornw/trustcall) 包以了解管理此问题的一种方法,并考虑评估(例如,使用 [LangSmith](https://docs.smith.langchain.com/tutorials/Developers/evaluation) 等工具)来帮助您调整行为。

使用文档集合还会将复杂性转移到列表上的内存**搜索**。`Store` 目前支持[语义搜索](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.query)和[按内容过滤](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.filter)。

最后,使用记忆集合可能会使向模型提供全面的上下文变得具有挑战性。虽然单个记忆可能遵循特定的模式,但这种结构可能无法捕获记忆之间的完整上下文或关系。因此,在使用这些记忆生成响应时,模型可能缺少在统一配置文件方法中更容易获得的重要上下文信息。

![](/oss/images/update-list.png)

无论采用何种内存管理方法,核心点是智能体将使用语义记忆来[支撑其响应](/oss/python/langchain/retrieval),这通常会导致更个性化和相关的交互。

### 情景记忆

[情景记忆](https://en.wikipedia.org/wiki/Episodic_memory),无论是在人类还是 AI 智能体中,都涉及回忆过去的事件或行动。[CoALA 论文](https://arxiv.org/pdf/2309.02427)很好地阐述了这一点:事实可以写入语义记忆,而*经历*可以写入情景记忆。对于 AI 智能体,情景记忆通常用于帮助智能体记住如何完成任务。

在实践中,情景记忆通常通过少样本示例提示来实现,智能体从过去的序列中学习以正确执行任务。有时"展示"比"告诉"更容易,LLM 从示例中学习得很好。少样本学习让您通过使用输入-输出示例更新提示来["编程"](https://x.com/karpathy/status/1627366413840322562)您的 LLM,以说明预期的行为。虽然可以使用各种最佳实践来生成少样本示例,但通常挑战在于根据用户输入选择最相关的示例。




请注意,内存[存储](/oss/python/langgraph/persistence#memory-store)只是将数据存储为少样本示例的一种方式。如果您想要更多的开发人员参与,或者将少样本示例更紧密地与您的评估工具联系起来,您还可以使用 [LangSmith 数据集](/langsmith/index-datasets-for-dynamic-few-shot-example-selection)来存储您的数据。然后可以开箱即用地使用动态少样本示例选择器来实现相同的目标。LangSmith 将为您索引数据集,并能够根据关键词相似性检索与用户输入最相关的少样本示例([使用类似 BM25 的算法](/langsmith/index-datasets-for-dynamic-few-shot-example-selection)进行基于关键词的相似性)。

请参阅此操作[视频](https://www.youtube.com/watch?v=37VaU7e7t5o),了解在 LangSmith 中使用动态少样本示例选择的示例用法。另外,请参阅此[博客文章](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/),展示少样本提示以提高工具调用性能,以及此[博客文章](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/),使用少样本示例将 LLM 与人类偏好对齐。




### 程序性记忆

[程序性记忆](https://en.wikipedia.org/wiki/Procedural_memory),无论是在人类还是 AI 智能体中,都涉及记住用于执行任务的规则。在人类中,程序性记忆就像执行任务的内化知识,例如通过基本的运动技能和平衡骑自行车。另一方面,情景记忆涉及回忆特定的经历,例如第一次成功骑自行车而不使用辅助轮或通过风景路线的难忘自行车骑行。对于 AI 智能体,程序性记忆是模型权重、智能体代码和智能体提示的组合,它们共同决定了智能体的功能。

在实践中,智能体修改其模型权重或重写其代码是相当不常见的。然而,智能体修改其自己的提示更为常见。

改进智能体指令的一种有效方法是通过["反思"](https://blog.langchain.dev/reflection-agents/)或元提示。这涉及使用其当前指令(例如,系统提示)以及最近的对话或明确的用户反馈来提示智能体。然后,智能体根据此输入改进其自己的指令。这种方法对于难以预先指定指令的任务特别有用,因为它允许智能体从其交互中学习和适应。

例如,我们构建了一个[推文生成器](https://www.youtube.com/watch?v=Vn8A3BxfplE),使用外部反馈和提示重写来为 Twitter 生成高质量的论文摘要。在这种情况下,特定的摘要提示很难*事先*指定,但用户批评生成的推文并提供有关如何改进摘要过程的反馈相当容易。

下面的伪代码展示了如何使用 LangGraph 内存[存储](/oss/python/langgraph/persistence#memory-store)来实现这一点,使用存储来保存提示,`update_instructions` 节点获取当前提示(以及从 `state["messages"]` 中捕获的与用户对话的反馈),更新提示,并将新提示保存回存储。然后,`call_model` 从存储中获取更新的提示并使用它来生成响应。

```python
# Node that *uses* the instructions
def call_model(state: State, store: BaseStore):
    namespace = ("agent_instructions", )
    instructions = store.get(namespace, key="agent_a")[0]
    # Application logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"])
    ...

# Node that updates instructions
def update_instructions(state: State, store: BaseStore):
    namespace = ("instructions",)
    instructions = store.search(namespace)[0]
    # Memory logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"], conversation=state["messages"])
    output = llm.invoke(prompt)
    new_instructions = output['new_instructions']
    store.put(("agent_instructions",), "agent_a", {"instructions": new_instructions})
    ...
```




![](/oss/images/update-instructions.png)

### 写入记忆

智能体写入记忆有两种主要方法:["在热路径上"](#in-the-hot-path)和["在后台"](#in-the-background)。

![](/oss/images/hot_path_vs_background.png)

#### 在热路径上

在运行时创建记忆既有优势也有挑战。从积极的方面来看,这种方法允许实时更新,使新记忆立即可用于后续交互。它还能够实现透明度,因为可以在创建和存储记忆时通知用户。

然而,这种方法也带来了挑战。如果智能体需要一个新工具来决定提交什么到内存,可能会增加复杂性。此外,推理要保存到内存的内容的过程可能会影响智能体延迟。最后,智能体必须在内存创建和其他职责之间进行多任务处理,可能会影响创建的记忆的数量和质量。

例如,ChatGPT 使用 [save_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) 工具将记忆作为内容字符串进行更新插入,决定是否以及如何在每条用户消息中使用此工具。请参阅我们的 [memory-agent](https://github.com/langchain-ai/memory-agent) 模板作为参考实现。

#### 在后台

将创建记忆作为单独的后台任务提供了几个优势。它消除了主应用程序中的延迟,将应用程序逻辑与内存管理分离,并允许智能体更专注地完成任务。这种方法还提供了在内存创建时间上的灵活性,以避免冗余工作。

然而,这种方法也有其自身的挑战。确定内存写入的频率变得至关重要,因为不频繁的更新可能会使其他线程缺少新的上下文。决定何时触发内存形成也很重要。常见的策略包括在设定的时间段后安排(如果发生新事件则重新安排)、使用 cron 计划或允许用户或应用程序逻辑手动触发。

请参阅我们的 [memory-service](https://github.com/langchain-ai/memory-template) 模板作为参考实现。

### 内存存储

LangGraph 将长期记忆作为 JSON 文档存储在[存储](/oss/python/langgraph/persistence#memory-store)中。每个记忆都组织在自定义 `namespace`(类似于文件夹)和不同的 `key`(如文件名)下。命名空间通常包括用户或组织 ID 或其他标签,使信息更容易组织。这种结构使记忆能够进行分层组织。然后通过内容过滤器支持跨命名空间搜索。

```python
from langgraph.store.memory import InMemoryStore


def embed(texts: list[str]) -> list[list[float]]:
    # Replace with an actual embedding function or LangChain embeddings object
    return [[1.0, 2.0] * len(texts)]


# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
store = InMemoryStore(index={"embed": embed, "dims": 2})
user_id = "my-user"
application_context = "chitchat"
namespace = (user_id, application_context)
store.put(
    namespace,
    "a-memory",
    {
        "rules": [
            "User likes short, direct language",
            "User only speaks English & python",
        ],
        "my-key": "my-value",
    },
)
# get the "memory" by ID
item = store.get(namespace, "a-memory")
# search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
items = store.search(
    namespace, filter={"my-key": "my-value"}, query="language preferences"
)
```




有关内存存储的更多信息,请参阅[持久化](/oss/python/langgraph/persistence#memory-store)指南。

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\concepts\memory.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
