---
title: 自定义中间件
---

通过在代理执行流程中的特定点实现钩子来构建自定义中间件。

## 钩子

中间件提供两种风格的钩子来拦截代理执行：

<CardGroup cols={2}>
  <Card title="节点式钩子" icon="share-nodes" href="#node-style-hooks">
    在特定执行点顺序运行。
  </Card>
  <Card title="包装式钩子" icon="container-storage" href="#wrap-style-hooks">
    围绕每个模型或工具调用运行。
  </Card>
</CardGroup>

### 节点式钩子

在特定执行点顺序运行。用于日志记录、验证和状态更新。

**可用钩子：**

- `before_agent` - 在代理启动之前（每次调用一次）
- `before_model` - 在每次模型调用之前
- `after_model` - 在每次模型响应之后
- `after_agent` - 在代理完成之后（每次调用一次）




**Example:**

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents.middleware import before_model, after_model, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any


@before_model(can_jump_to=["end"])
def check_message_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    if len(state["messages"]) >= 50:
        return {
            "messages": [AIMessage("Conversation limit reached.")],
            "jump_to": "end"
        }
    return None

@after_model
def log_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"Model returned: {state['messages'][-1].content}")
    return None
```

</Tab>

<Tab title="Class">

```python
from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class MessageLimitMiddleware(AgentMiddleware):
    def __init__(self, max_messages: int = 50):
        super().__init__()
        self.max_messages = max_messages

    @hook_config(can_jump_to=["end"])
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        if len(state["messages"]) == self.max_messages:
            return {
                "messages": [AIMessage("Conversation limit reached.")],
                "jump_to": "end"
            }
        return None

    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"Model returned: {state['messages'][-1].content}")
        return None
```

</Tab>
</Tabs>





### 包装式钩子

拦截执行并控制何时调用处理程序。用于重试、缓存和转换。

您决定处理程序是否被调用零次（短路）、一次（正常流程）或多次（重试逻辑）。

**可用钩子：**

- `wrap_model_call` - 围绕每个模型调用
- `wrap_tool_call` - 围绕每个工具调用




**Example:**

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable


@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"Retry {attempt + 1}/3 after error: {e}")
```

</Tab>

<Tab title="Class">

```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class RetryMiddleware(AgentMiddleware):
    def __init__(self, max_retries: int = 3):
        super().__init__()
        self.max_retries = max_retries

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        for attempt in range(self.max_retries):
            try:
                return handler(request)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Retry {attempt + 1}/{self.max_retries} after error: {e}")
```

</Tab>
</Tabs>





## 创建中间件

您可以通过两种方式创建中间件：

<CardGroup cols={2}>
  <Card title="基于装饰器的中间件" icon="at" href="#decorator-based-middleware">
    对于单钩子中间件快速简单。使用装饰器包装单个函数。
  </Card>
  <Card title="基于类的中间件" icon="brackets-curly" href="#class-based-middleware">
    对于具有多个钩子或配置的复杂中间件更强大。
  </Card>
</CardGroup>

### 基于装饰器的中间件

对于单钩子中间件快速简单。使用装饰器包装单个函数。

**可用装饰器：**

**节点式：**
- [`@before_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_agent) - 在代理启动之前运行（每次调用一次）
- [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) - 在每次模型调用之前运行
- [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) - 在每次模型响应之后运行
- [`@after_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_agent) - 在代理完成之后运行（每次调用一次）

**包装式：**
- [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) - 用自定义逻辑包装每个模型调用
- [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) - 用自定义逻辑包装每个工具调用

**便利性：**
- [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) - 生成动态系统提示

**Example:**

```python
from langchain.agents.middleware import before_model, wrap_model_call
from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse
from langchain.agents import create_agent
from langgraph.runtime import Runtime
from typing import Any, Callable


@before_model
def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"About to call model with {len(state['messages'])} messages")
    return None

@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"Retry {attempt + 1}/3 after error: {e}")

agent = create_agent(
    model="gpt-4o",
    middleware=[log_before_model, retry_model],
    tools=[...],
)
```

**何时使用装饰器：**
- 需要单个钩子
- 没有复杂配置
- 快速原型设计

### 基于类的中间件

对于具有多个钩子或配置的复杂中间件更强大。当您需要为同一钩子定义同步和异步实现，或者想要在单个中间件中组合多个钩子时，请使用类。

**Example:**

```python
from langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse
from langgraph.runtime import Runtime
from typing import Any, Callable

class LoggingMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"About to call model with {len(state['messages'])} messages")
        return None

    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"Model returned: {state['messages'][-1].content}")
        return None

agent = create_agent(
    model="gpt-4o",
    middleware=[LoggingMiddleware()],
    tools=[...],
)
```

**何时使用类：**
- 为同一钩子定义同步和异步实现
- 单个中间件中需要多个钩子
- 需要复杂配置（例如，可配置的阈值、自定义模型）
- 在项目间重用，具有初始化时配置





## 自定义状态模式

中间件可以使用自定义属性扩展代理的状态。

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents.middleware import AgentState, before_model, after_model
from typing_extensions import NotRequired
from typing import Any
from langgraph.runtime import Runtime

class CustomState(AgentState):
    model_call_count: NotRequired[int]
    user_id: NotRequired[str]

@before_model(state_schema=CustomState, can_jump_to=["end"])
def check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:
    count = state.get("model_call_count", 0)
    if count > 10:
        return {"jump_to": "end"}
    return None

@after_model(state_schema=CustomState)
def increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:
    return {"model_call_count": state.get("model_call_count", 0) + 1}

agent = create_agent(
    model="gpt-4o",
    middleware=[check_call_limit, increment_counter],
    tools=[...],
)

# Invoke with custom state
result = agent.invoke({
    "messages": [HumanMessage("Hello")],
    "model_call_count": 0,
    "user_id": "user-123",
})
```

</Tab>

<Tab title="Class">

```python
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CustomState(AgentState):
    model_call_count: NotRequired[int]
    user_id: NotRequired[str]

class CallCounterMiddleware(AgentMiddleware[CustomState]):
    state_schema = CustomState

    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        count = state.get("model_call_count", 0)
        if count > 10:
            return {"jump_to": "end"}
        return None

    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        return {"model_call_count": state.get("model_call_count", 0) + 1}

agent = create_agent(
    model="gpt-4o",
    middleware=[CallCounterMiddleware()],
    tools=[...],
)

# Invoke with custom state
result = agent.invoke({
    "messages": [HumanMessage("Hello")],
    "model_call_count": 0,
    "user_id": "user-123",
})
```

</Tab>
</Tabs>





## 执行顺序

使用多个中间件时，了解它们如何执行：

```python
agent = create_agent(
    model="gpt-4o",
    middleware=[middleware1, middleware2, middleware3],
    tools=[...],
)
```




<Accordion title="执行流程">

**Before 钩子按顺序运行：**
1. `middleware1.before_agent()`
2. `middleware2.before_agent()`
3. `middleware3.before_agent()`

__代理循环开始__

4. `middleware1.before_model()`
5. `middleware2.before_model()`
6. `middleware3.before_model()`

**Wrap 钩子像函数调用一样嵌套：**

7. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model

**After 钩子按相反顺序运行：**

8. `middleware3.after_model()`
9. `middleware2.after_model()`
10. `middleware1.after_model()`

__代理循环结束__

11. `middleware3.after_agent()`
12. `middleware2.after_agent()`
13. `middleware1.after_agent()`

</Accordion>

**关键规则：**
- `before_*` 钩子：从第一个到最后一个
- `after_*` 钩子：从最后一个到第一个（相反）
- `wrap_*` 钩子：嵌套（第一个中间件包装所有其他）

## 代理跳转

要从中间件提前退出，返回带有 `jump_to` 的字典：

**可用跳转目标：**
- `'end'`: 跳转到代理执行的末尾（或第一个 `after_agent` 钩子）
- `'tools'`: 跳转到工具节点
- `'model'`: 跳转到模型节点（或第一个 `before_model` 钩子）

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents.middleware import after_model, hook_config, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any


@after_model
@hook_config(can_jump_to=["end"])
def check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    last_message = state["messages"][-1]
    if "BLOCKED" in last_message.content:
        return {
            "messages": [AIMessage("I cannot respond to that request.")],
            "jump_to": "end"
        }
    return None
```

</Tab>

<Tab title="Class">

```python
from langchain.agents.middleware import AgentMiddleware, hook_config, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class BlockedContentMiddleware(AgentMiddleware):
    @hook_config(can_jump_to=["end"])
    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        last_message = state["messages"][-1]
        if "BLOCKED" in last_message.content:
            return {
                "messages": [AIMessage("I cannot respond to that request.")],
                "jump_to": "end"
            }
        return None
```

</Tab>
</Tabs>





## 最佳实践

1. 保持中间件专注 - 每个应该做好一件事
2. 优雅地处理错误 - 不要让中间件错误使代理崩溃
3. **使用适当的钩子类型**：
    - 节点式用于顺序逻辑（日志记录、验证）
    - 包装式用于控制流（重试、回退、缓存）
4. 清楚地记录任何自定义状态属性
5. 在集成之前独立地对中间件进行单元测试
6. 考虑执行顺序 - 将关键中间件放在列表的第一位
7. 尽可能使用内置中间件

## 示例

### 动态模型选择

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable


complex_model = init_chat_model("gpt-4o")
simple_model = init_chat_model("gpt-4o-mini")

@wrap_model_call
def dynamic_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    # Use different model based on conversation length
   if len(request.messages) > 10:
       model = complex_model
    else:
       model = simple_model
    return handler(request.override(model=model))
```

</Tab>

<Tab title="Class">

```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

complex_model = init_chat_model("gpt-4o")
simple_model = init_chat_model("gpt-4o-mini")

class DynamicModelMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Use different model based on conversation length
        if len(request.messages) > 10:
           model = complex_model
        else:
           model = simple_model
        return handler(request.override(model=model))
```

</Tab>
</Tabs>





### 工具调用监控

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents.middleware import wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest
from langchain.messages import ToolMessage
from langgraph.types import Command
from typing import Callable


@wrap_tool_call
def monitor_tool(
    request: ToolCallRequest,
    handler: Callable[[ToolCallRequest], ToolMessage | Command],
) -> ToolMessage | Command:
    print(f"Executing tool: {request.tool_call['name']}")
    print(f"Arguments: {request.tool_call['args']}")
    try:
        result = handler(request)
        print(f"Tool completed successfully")
        return result
    except Exception as e:
        print(f"Tool failed: {e}")
        raise
```

</Tab>

<Tab title="Class">

```python
from langchain.tools.tool_node import ToolCallRequest
from langchain.agents.middleware import AgentMiddleware
from langchain.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

class ToolMonitoringMiddleware(AgentMiddleware):
    def wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        print(f"Executing tool: {request.tool_call['name']}")
        print(f"Arguments: {request.tool_call['args']}")
        try:
            result = handler(request)
            print(f"Tool completed successfully")
            return result
        except Exception as e:
            print(f"Tool failed: {e}")
            raise
```

</Tab>
</Tabs>





### 动态选择工具

在运行时选择相关工具以提高性能和准确性。

**好处：**
- **更短的提示** - 通过仅暴露相关工具来降低复杂性
- **更好的准确性** - 模型从更少的选项中选择正确
- **权限控制** - 根据用户访问权限动态过滤工具

<Tabs>
<Tab title="Decorator">

```python
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable


@wrap_model_call
def select_tools(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    """Middleware to select relevant tools based on state/context."""
    # Select a small, relevant subset of tools based on state/context
    relevant_tools = select_relevant_tools(request.state, request.runtime)
    return handler(request.override(tools=relevant_tools))

agent = create_agent(
    model="gpt-4o",
    tools=all_tools,  # All available tools need to be registered upfront
    middleware=[select_tools],
)
```

</Tab>

<Tab title="Class">

```python
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable


class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """Middleware to select relevant tools based on state/context."""
        # Select a small, relevant subset of tools based on state/context
        relevant_tools = select_relevant_tools(request.state, request.runtime)
        return handler(request.override(tools=relevant_tools))

agent = create_agent(
    model="gpt-4o",
    tools=all_tools,  # All available tools need to be registered upfront
    middleware=[ToolSelectorMiddleware()],
)
```

</Tab>
</Tabs>





## 其他资源

- [中间件 API 参考](https://reference.langchain.com/python/langchain/middleware/)
- [内置中间件](/oss/python/langchain/middleware/built-in)
- [测试代理](/oss/python/langchain/test)

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\langchain\middleware\custom.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
