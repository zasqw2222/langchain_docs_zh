---
title: 测试
---



代理应用让 LLM 决定自己的下一步来解决问题。这种灵活性很强大，但模型的黑盒性质使得很难预测代理某一部分的调整将如何影响其他部分。要构建生产就绪的代理，彻底的测试是必不可少的。

测试代理有几种方法：
- [单元测试](#unit-testing)使用内存假对象在隔离环境中测试代理的小型、确定性部分，以便您可以快速且确定地断言确切行为。


- [集成测试](#integration-testing)使用真实网络调用测试代理，以确认组件协同工作、凭据和模式匹配，以及延迟可接受。

代理应用往往更依赖集成测试，因为它们将多个组件链接在一起，并且必须处理由于 LLM 的非确定性性质而导致的不稳定性。

## 单元测试

### 模拟聊天模型

对于不需要 API 调用的逻辑，您可以使用内存存根来模拟响应。

LangChain 提供 [`GenericFakeChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.fake_chat_models.GenericFakeChatModel.html) 来模拟文本响应。它接受响应迭代器（AIMessages 或字符串），每次调用返回一个。它支持常规和流式使用。

```python
from langchain_core.language_models.fake_chat_models import GenericFakeChatModel

model = GenericFakeChatModel(messages=iter([
    AIMessage(content="", tool_calls=[ToolCall(name="foo", args={"bar": "baz"}, id="call_1")]),
    "bar"
]))

model.invoke("hello")
# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])
```

如果我们再次调用模型，它将返回迭代器中的下一个项目：

```python
model.invoke("hello, again!")
# AIMessage(content='bar', ...)
```

### InMemorySaver 检查点保存器

为了在测试期间启用持久性，您可以使用 [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) 检查点保存器。这允许您模拟多轮对话以测试依赖于状态的行为：

```python
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model,
    tools=[],
    checkpointer=InMemorySaver()
)

# 第一次调用
agent.invoke(HumanMessage(content="I live in Sydney, Australia."))

# 第二次调用：第一条消息已持久化（悉尼位置），因此模型返回 GMT+10 时间
agent.invoke(HumanMessage(content="What's my local time?"))
```


## 集成测试

许多代理行为只有在使用真实 LLM 时才会出现，例如代理决定调用哪个工具、如何格式化响应，或者提示修改是否影响整个执行轨迹。LangChain 的 [`agentevals`](https://github.com/langchain-ai/agentevals) 包提供了专门设计用于使用实时模型测试代理轨迹的评估器。

AgentEvals 允许您通过执行**轨迹匹配**或使用**LLM 评判器**轻松评估代理的轨迹（消息的确切序列，包括工具调用）：

<Card title="轨迹匹配" icon="equals" arrow="true" href="#trajectory-match-evaluator">
为给定输入硬编码参考轨迹，并通过逐步比较验证运行。

适用于测试明确定义的工作流，您知道预期行为。当您对应该调用哪些工具以及调用顺序有特定期望时使用。这种方法具有确定性、快速且成本效益高，因为它不需要额外的 LLM 调用。
</Card>

<Card title="LLM 作为评判器" icon="gavel" arrow="true" href="#llm-as-judge-evaluator">
使用 LLM 定性验证代理的执行轨迹。"评判器" LLM 根据提示标准（可以包括参考轨迹）审查代理的决策。

更灵活，可以评估效率和适当性等细微方面，但需要 LLM 调用且确定性较低。当您想要评估代理轨迹的整体质量和合理性而不需要严格的工具调用或顺序要求时使用。
</Card>

### 安装 AgentEvals

```bash
pip install agentevals
```




或者，直接克隆 [AgentEvals 仓库](https://github.com/langchain-ai/agentevals)。

### 轨迹匹配评估器

AgentEvals 提供 `create_trajectory_match_evaluator` 函数来将代理的轨迹与参考轨迹进行匹配。有四种模式可供选择：



| 模式 | 描述 | 用例 |
|------|-------------|----------|
| `strict` | 相同顺序的消息和工具调用的完全匹配 | 测试特定序列（例如，在授权之前进行策略查找） |
| `unordered` | 允许以任何顺序调用相同的工具 | 当顺序不重要时验证信息检索 |
| `subset` | 代理仅调用参考中的工具（无额外工具） | 确保代理不超过预期范围 |
| `superset` | 代理至少调用参考工具（允许额外工具） | 验证已采取所需的最小操作 |

<Accordion title="严格匹配">

`strict` 模式确保轨迹包含相同顺序的相同消息和工具调用，尽管它允许消息内容存在差异。当您需要强制执行特定操作序列时，这很有用，例如要求在授权操作之前进行策略查找。

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langchain.messages import HumanMessage, AIMessage, ToolMessage
from agentevals.trajectory.match import create_trajectory_match_evaluator


@tool
def get_weather(city: str):
    """Get weather information for a city."""
    return f"It's 75 degrees and sunny in {city}."

agent = create_agent("gpt-4o", tools=[get_weather])

evaluator = create_trajectory_match_evaluator(  # [!code highlight]
    trajectory_match_mode="strict",  # [!code highlight]
)  # [!code highlight]

def test_weather_tool_called_strict():
    result = agent.invoke({
        "messages": [HumanMessage(content="What's the weather in San Francisco?")]
    })

    reference_trajectory = [
        HumanMessage(content="What's the weather in San Francisco?"),
        AIMessage(content="", tool_calls=[
            {"id": "call_1", "name": "get_weather", "args": {"city": "San Francisco"}}
        ]),
        ToolMessage(content="It's 75 degrees and sunny in San Francisco.", tool_call_id="call_1"),
        AIMessage(content="The weather in San Francisco is 75 degrees and sunny."),
    ]

    evaluation = evaluator(
        outputs=result["messages"],
        reference_outputs=reference_trajectory
    )
    # {
    #     'key': 'trajectory_strict_match',
    #     'score': True,
    #     'comment': None,
    # }
    assert evaluation["score"] is True
```




</Accordion>

<Accordion title="无序匹配">

`unordered` 模式允许以任何顺序调用相同的工具，当您想要验证是否检索了特定信息但不关心顺序时，这很有用。例如，代理可能需要检查城市的天气和事件，但顺序并不重要。

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langchain.messages import HumanMessage, AIMessage, ToolMessage
from agentevals.trajectory.match import create_trajectory_match_evaluator


@tool
def get_weather(city: str):
    """Get weather information for a city."""
    return f"It's 75 degrees and sunny in {city}."

@tool
def get_events(city: str):
    """Get events happening in a city."""
    return f"Concert at the park in {city} tonight."

agent = create_agent("gpt-4o", tools=[get_weather, get_events])

evaluator = create_trajectory_match_evaluator(  # [!code highlight]
    trajectory_match_mode="unordered",  # [!code highlight]
)  # [!code highlight]

def test_multiple_tools_any_order():
    result = agent.invoke({
        "messages": [HumanMessage(content="What's happening in SF today?")]
    })

    # 参考显示工具调用的顺序与实际执行不同
    reference_trajectory = [
        HumanMessage(content="What's happening in SF today?"),
        AIMessage(content="", tool_calls=[
            {"id": "call_1", "name": "get_events", "args": {"city": "SF"}},
            {"id": "call_2", "name": "get_weather", "args": {"city": "SF"}},
        ]),
        ToolMessage(content="Concert at the park in SF tonight.", tool_call_id="call_1"),
        ToolMessage(content="It's 75 degrees and sunny in SF.", tool_call_id="call_2"),
        AIMessage(content="Today in SF: 75 degrees and sunny with a concert at the park tonight."),
    ]

    evaluation = evaluator(
        outputs=result["messages"],
        reference_outputs=reference_trajectory,
    )
    # {
    #     'key': 'trajectory_unordered_match',
    #     'score': True,
    # }
    assert evaluation["score"] is True
```




</Accordion>

<Accordion title="子集和超集匹配">

`superset` 和 `subset` 模式匹配部分轨迹。`superset` 模式验证代理至少调用了参考轨迹中的工具，允许额外的工具调用。`subset` 模式确保代理没有调用参考中工具之外的任何工具。

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langchain.messages import HumanMessage, AIMessage, ToolMessage
from agentevals.trajectory.match import create_trajectory_match_evaluator


@tool
def get_weather(city: str):
    """Get weather information for a city."""
    return f"It's 75 degrees and sunny in {city}."

@tool
def get_detailed_forecast(city: str):
    """Get detailed weather forecast for a city."""
    return f"Detailed forecast for {city}: sunny all week."

agent = create_agent("gpt-4o", tools=[get_weather, get_detailed_forecast])

evaluator = create_trajectory_match_evaluator(  # [!code highlight]
    trajectory_match_mode="superset",  # [!code highlight]
)  # [!code highlight]

def test_agent_calls_required_tools_plus_extra():
    result = agent.invoke({
        "messages": [HumanMessage(content="What's the weather in Boston?")]
    })

    # 参考仅需要 get_weather，但代理可能调用其他工具
    reference_trajectory = [
        HumanMessage(content="What's the weather in Boston?"),
        AIMessage(content="", tool_calls=[
            {"id": "call_1", "name": "get_weather", "args": {"city": "Boston"}},
        ]),
        ToolMessage(content="It's 75 degrees and sunny in Boston.", tool_call_id="call_1"),
        AIMessage(content="The weather in Boston is 75 degrees and sunny."),
    ]

    evaluation = evaluator(
        outputs=result["messages"],
        reference_outputs=reference_trajectory,
    )
    # {
    #     'key': 'trajectory_superset_match',
    #     'score': True,
    #     'comment': None,
    # }
    assert evaluation["score"] is True
```




</Accordion>

<Info>
您还可以设置 `tool_args_match_mode` 属性和/或 `tool_args_match_overrides` 来自定义评估器如何考虑实际轨迹与参考中工具调用之间的相等性。默认情况下，只有具有相同参数和相同工具的工具调用才被视为相等。访问[仓库](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes)了解更多详细信息。


</Info>

### LLM 作为评判器评估器

您还可以使用 `create_trajectory_llm_as_judge` 函数使用 LLM 评估代理的执行路径。与轨迹匹配评估器不同，它不需要参考轨迹，但如果可用，可以提供参考轨迹。



<Accordion title="无参考轨迹">
```python
from langchain.agents import create_agent
from langchain.tools import tool
from langchain.messages import HumanMessage, AIMessage, ToolMessage
from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT


@tool
def get_weather(city: str):
    """Get weather information for a city."""
    return f"It's 75 degrees and sunny in {city}."

agent = create_agent("gpt-4o", tools=[get_weather])

evaluator = create_trajectory_llm_as_judge(  # [!code highlight]
    model="openai:o3-mini",  # [!code highlight]
    prompt=TRAJECTORY_ACCURACY_PROMPT,  # [!code highlight]
)  # [!code highlight]

def test_trajectory_quality():
    result = agent.invoke({
        "messages": [HumanMessage(content="What's the weather in Seattle?")]
    })

    evaluation = evaluator(
        outputs=result["messages"],
    )
    # {
    #     'key': 'trajectory_accuracy',
    #     'score': True,
    #     'comment': 'The provided agent trajectory is reasonable...'
    # }
    assert evaluation["score"] is True
```


</Accordion>

<Accordion title="带参考轨迹">

如果您有参考轨迹，可以在提示中添加额外的变量并传入参考轨迹。下面，我们使用预构建的 `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` 提示并配置 `reference_outputs` 变量：

```python
evaluator = create_trajectory_llm_as_judge(
    model="openai:o3-mini",
    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,
)
evaluation = judge_with_reference(
    outputs=result["messages"],
    reference_outputs=reference_trajectory,
)
```




</Accordion>

<Info>
有关如何配置 LLM 评估轨迹的更多信息，请访问[仓库](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge)。
</Info>

### 异步支持

所有 `agentevals` 评估器都支持 Python asyncio。对于使用工厂函数的评估器，通过在函数名中的 `create_` 后添加 `async` 来使用异步版本。

<Accordion title="异步评判器和评估器示例">

```python
from agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT
from agentevals.trajectory.match import create_async_trajectory_match_evaluator

async_judge = create_async_trajectory_llm_as_judge(
    model="openai:o3-mini",
    prompt=TRAJECTORY_ACCURACY_PROMPT,
)

async_evaluator = create_async_trajectory_match_evaluator(
    trajectory_match_mode="strict",
)

async def test_async_evaluation():
    result = await agent.ainvoke({
        "messages": [HumanMessage(content="What's the weather?")]
    })

    evaluation = await async_judge(outputs=result["messages"])
    assert evaluation["score"] is True
```

</Accordion>



## LangSmith 集成

为了跟踪随时间推移的实验，您可以将评估器结果记录到 [LangSmith](https://smith.langchain.com/)，这是一个用于构建生产级 LLM 应用的平台，包括跟踪、评估和实验工具。

首先，通过设置所需的环境变量来设置 LangSmith：

```bash
export LANGSMITH_API_KEY="your_langsmith_api_key"
export LANGSMITH_TRACING="true"
```

LangSmith 提供两种主要的运行评估方法：[pytest](/langsmith/pytest) 集成和 `evaluate` 函数。

<Accordion title="使用 pytest 集成">

```python
import pytest
from langsmith import testing as t
from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

trajectory_evaluator = create_trajectory_llm_as_judge(
    model="openai:o3-mini",
    prompt=TRAJECTORY_ACCURACY_PROMPT,
)

@pytest.mark.langsmith
def test_trajectory_accuracy():
    result = agent.invoke({
        "messages": [HumanMessage(content="What's the weather in SF?")]
    })

    reference_trajectory = [
        HumanMessage(content="What's the weather in SF?"),
        AIMessage(content="", tool_calls=[
            {"id": "call_1", "name": "get_weather", "args": {"city": "SF"}},
        ]),
        ToolMessage(content="It's 75 degrees and sunny in SF.", tool_call_id="call_1"),
        AIMessage(content="The weather in SF is 75 degrees and sunny."),
    ]

    # 将输入、输出和参考输出记录到 LangSmith
    t.log_inputs({})
    t.log_outputs({"messages": result["messages"]})
    t.log_reference_outputs({"messages": reference_trajectory})

    trajectory_evaluator(
        outputs=result["messages"],
        reference_outputs=reference_trajectory
    )
```

使用 pytest 运行评估：

```bash
pytest test_trajectory.py --langsmith-output
```

结果将自动记录到 LangSmith。

</Accordion>

<Accordion title="使用 evaluate 函数">

或者，您可以在 LangSmith 中创建数据集并使用 `evaluate` 函数：

```python
from langsmith import Client
from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

client = Client()

trajectory_evaluator = create_trajectory_llm_as_judge(
    model="openai:o3-mini",
    prompt=TRAJECTORY_ACCURACY_PROMPT,
)

def run_agent(inputs):
    """返回轨迹消息的代理函数。"""
    return agent.invoke(inputs)["messages"]

experiment_results = client.evaluate(
    run_agent,
    data="your_dataset_name",
    evaluators=[trajectory_evaluator]
)
```

结果将自动记录到 LangSmith。


</Accordion>

<Tip>
要了解有关评估代理的更多信息，请参阅 [LangSmith 文档](/langsmith/pytest)。
</Tip>




## 记录和重放 HTTP 调用

调用真实 LLM API 的集成测试可能很慢且昂贵，尤其是在 CI/CD 管道中频繁运行时。我们建议使用库来记录 HTTP 请求和响应，然后在后续运行中重放它们，而无需进行实际的网络调用。

您可以使用 [`vcrpy`](https://pypi.org/project/vcrpy/1.5.2/) 来实现这一点。如果您使用 `pytest`，[`pytest-recording` 插件](https://pypi.org/project/pytest-recording/)提供了一种简单的方法，只需最少的配置即可启用此功能。请求/响应记录在 cassettes 中，然后在后续运行中使用这些 cassettes 来模拟真实的网络调用。

设置您的 `conftest.py` 文件以从 cassettes 中过滤敏感信息：

```py conftest.py
import pytest

@pytest.fixture(scope="session")
def vcr_config():
    return {
        "filter_headers": [
            ("authorization", "XXXX"),
            ("x-api-key", "XXXX"),
            # ... 您想要屏蔽的其他标头
        ],
        "filter_query_parameters": [
            ("api_key", "XXXX"),
            ("key", "XXXX"),
        ],
    }
```

然后配置您的项目以识别 `vcr` 标记：

<CodeGroup>
```ini pytest.ini
[pytest]
markers =
    vcr: record/replay HTTP via VCR
addopts = --record-mode=once
```
```toml pyproject.toml
[tool.pytest.ini_options]
markers = [
  "vcr: record/replay HTTP via VCR"
]
addopts = "--record-mode=once"
```
</CodeGroup>

<Info>
`--record-mode=once` 选项在第一次运行时记录 HTTP 交互，并在后续运行中重放它们。
</Info>

现在，只需用 `vcr` 标记装饰您的测试：

```python
@pytest.mark.vcr()
def test_agent_trajectory():
    # ...
```

第一次运行此测试时，您的代理将进行真实的网络调用，pytest 将在 `tests/cassettes` 目录中生成一个 cassette 文件 `test_agent_trajectory.yaml`。后续运行将使用该 cassette 来模拟真实的网络调用，前提是代理的请求与之前的运行相比没有变化。如果它们发生变化，测试将失败，您需要删除 cassette 并重新运行测试以记录新的交互。

<Warning>
当您修改提示、添加新工具或更改预期轨迹时，您保存的 cassettes 将变得过时，您现有的测试**将失败**。您应该删除相应的 cassette 文件并重新运行测试以记录新的交互。
</Warning>

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss\langchain\test.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
